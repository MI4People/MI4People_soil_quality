{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Extraction of BigEarthNet-v1.0 Dataset\n",
    "This notebook creates a pickle file which contains all labels of the BigEarthNet-v1.0 (BEN) dataset in a dict format. To do so, the image label JSON files of the first 50k image bucket folders on AWS are loaded. (Note: The assumption is that those first 50k image bucket folders are sufficient to capture all labels contained in the dataset.) Next, the labels are extracted and saved in a local Pickle file called `ben_labels`.\n",
    "\n",
    "It is sufficient to run this notebook only once to extract the labels to the Pickle file as the Pickle file `ben_labels` is stored in the repo as well. The file can be read via `src.globals.LABELS_TO_INDS`, so you should not need to run this here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "To run the notebook, you need to have stored your AWS credentials in the file `aws_credentials.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanja\\PycharmProjects\\mi4people-soil-quality\\src\\globals.py:21: UserWarning: If you want to make use of global variable LABELS_TO_INDS containing all labels for the BigEarthNet-v1.0 (BEN) dataset, you need to execute the notebook `bigearthnet_label_creation.ipnyb` first.\n",
      "  warnings.warn(\"If you want to make use of global variable LABELS_TO_INDS containing all labels for the BigEarthNet-v1.0 (BEN) dataset, you need to execute the notebook `bigearthnet_label_creation.ipnyb` first.\")\n"
     ]
    }
   ],
   "source": [
    "# You only need to run this code snippet if your credentials are not set within your environment yet\n",
    "from src.infrastructure.aws_infrastructure import get_aws_credentials, set_s3_credentials\n",
    "\n",
    "aws_credentials = get_aws_credentials()\n",
    "set_s3_credentials(aws_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  590326\n",
      "First folder name:  s3://mi4people-soil-project/BigEarthNet-v1.0/S2A_MSIL2A_20170613T101031_0_45\n"
     ]
    }
   ],
   "source": [
    "import src.data.general_datapipes as pipes\n",
    "\n",
    "# Get a list with all image folder names in s3 \n",
    "folder_list = list(pipes.get_s3_folder_content())\n",
    "print(\"Length: \", len(folder_list))\n",
    "print(\"First folder name: \", folder_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanja\\Anaconda3\\envs\\soil_quality\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\utils\\common.py:145: UserWarning: Lambda function is not supported by pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n",
      "2446it [12:12,  3.34it/s]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import fsspec\n",
    "from pathlib import Path\n",
    "\n",
    "import torchdata.datapipes as dp\n",
    "\n",
    "import src.data.bigearthnet_datapipes as ben_pipes\n",
    "from src.globals import PROJECT_DIR\n",
    "\n",
    "# Iterate through first 50000 image folders (Assumption is that this will catch all different BigEarthNet Labels)\n",
    "pipe = dp.iter.IterableWrapper(folder_list[:50000])\n",
    "# Check subfolders\n",
    "pipe = pipe.list_files_by_fsspec()\n",
    "pipe = pipe.groupby(group_key_fn=ben_pipes.group_key_by_folder, group_size=13)\n",
    "pipe = pipe.map(ben_pipes.chunk_to_dataloader_dict)\n",
    "pipe = pipe.map(lambda x: x[\"label\"])\n",
    "pipe = pipe.map(lambda x: json.loads(fsspec.open(x, mode=\"r\").open().read())[\"labels\"])\n",
    "\n",
    "classes = list()\n",
    "for js in tqdm.tqdm(pipe):\n",
    "    for label in js:\n",
    "        classes.append(label)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(classes)\n",
    "# Classes sorted alphabetically\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "classes_to_ind = {i: cls for (i, cls) in enumerate(classes)}\n",
    "\n",
    "print(counter)\n",
    "print(classes)\n",
    "print(classes_to_ind)\n",
    "\n",
    "# Save dict as pickle file in data folder of this project\n",
    "with open(\"C:/Users/tanja/PycharmProjects/mi4people-soil-quality/data/raw/ben_labels.pickle\", \"wb\") as p_out:\n",
    "    pickle.dump(classes_to_ind, p_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soil_quality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
