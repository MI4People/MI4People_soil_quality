{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d7724c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Calculate temporal statistical metrics from cloud-masked Sentinel 2 data on small areas (image chips) \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0191d46-d00b-475e-ac21-07054b338978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import pystac\n",
    "import pystac_client\n",
    "from pystac_client import Client\n",
    "import planetary_computer\n",
    "import rasterio\n",
    "from shapely.geometry import Point, Polygon\n",
    "import rioxarray\n",
    "import stackstac\n",
    "from tqdm import tqdm\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b5ecf-b759-4f0a-a0a9-5f7c392acf53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee3c2c49-3775-4be2-b82a-9ffece66012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(pystac_lient_url: str, datetimes: str, chip: pd.core.series.Series):\n",
    "\n",
    "    stac = pystac_client.Client.open(pystac_lient_url)\n",
    "    search = stac.search(\n",
    "        intersects=dict(type=\"Point\", coordinates=[chip.longitude, chip.latitude]),\n",
    "        datetime=datetimes,\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        limit=500,  # fetch items in batches of 500\n",
    "        query={\n",
    "            \"eo:cloud_cover\": {\"lt\": 80},\n",
    "            # we only work with small chip areas and only use data from one zone\n",
    "            # this avoids duplicates while we should not miss any data\n",
    "            \"proj:epsg\": {\"eq\": int(chip.epsg)},\n",
    "        },\n",
    "    )\n",
    "    candidate_items = list(search.get_items())\n",
    "    if len(candidate_items) == 0:\n",
    "        # print(f'NO ITEMS FOUND')\n",
    "        return None, None\n",
    "    ## scenes db for overview\n",
    "    ids = []\n",
    "    geometries = []\n",
    "    cloud_cover = []\n",
    "    pt = Point([chip.longitude, chip.latitude])\n",
    "\n",
    "    for item in candidate_items:\n",
    "        ids.append(item.id)\n",
    "        # poly = Polygon.from_bounds(*item.bbox)\n",
    "        assert len(item.geometry[\"coordinates\"]) == 1\n",
    "        poly = Polygon(item.geometry[\"coordinates\"][0])\n",
    "        geometries.append(poly)\n",
    "        cloud_cover.append(item.properties[\"eo:cloud_cover\"])\n",
    "\n",
    "    scenes = pd.DataFrame(\n",
    "        {\"id\": ids, \"geometry\": geometries, \"cloud_cover\": cloud_cover}\n",
    "    )\n",
    "\n",
    "    # https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/naming-convention\n",
    "    candidate_scenes = pd.concat(\n",
    "        [\n",
    "            scenes,\n",
    "            scenes.id.str.split(\"_\", expand=True).rename(\n",
    "                {\n",
    "                    0: \"mission\",\n",
    "                    1: \"product_level\",\n",
    "                    2: \"datatake_sensing_start\",\n",
    "                    3: \"relative_orbit\",\n",
    "                    4: \"tile\",\n",
    "                    5: \"product_discriminator\",\n",
    "                },\n",
    "                axis=1,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    candidate_scenes[\"date\"] = pd.to_datetime(\n",
    "        candidate_scenes[\"datatake_sensing_start\"].str[:8]\n",
    "    )\n",
    "    candidate_scenes[\"chip_id\"] = chip[\"chip_id\"]\n",
    "\n",
    "    candidate_scenes[\"unique_acquisitions\"] = False\n",
    "    keep = candidate_scenes.groupby([\"mission\", \"date\"]).first()[\"id\"]\n",
    "    candidate_scenes.loc[scenes[\"id\"].isin(keep), \"unique_acquisitions\"] = True\n",
    "\n",
    "    scenes = candidate_scenes.query(\"unique_acquisitions\")\n",
    "    items = [it for it in candidate_items if it.id in scenes[\"id\"].values]\n",
    "    assert len(items) == scenes.shape[0]\n",
    "    return items, candidate_scenes\n",
    "\n",
    "\n",
    "def mask_bands_with_valid_pixel_mask(stack_bands, stack_valid_pixel_mask):\n",
    "    return stack_bands.where(stack_valid_pixel_mask == 1, np.nan)\n",
    "\n",
    "\n",
    "def filter_times_given_periods(datetime_list, periods):\n",
    "    filtered_dates = []\n",
    "    for period in periods:  # example format of period: '2018-01-01/2018-03-31'\n",
    "        dt_from = np.datetime64(period.split(\"/\")[0])\n",
    "        dt_to = np.datetime64(period.split(\"/\")[1])\n",
    "        filter_dates = filter(lambda d: (d >= dt_from) & (d < dt_to + 1), datetime_list)\n",
    "        filtered_dates += list(filter_dates)\n",
    "    return filtered_dates\n",
    "\n",
    "\n",
    "def quantiles_from_masked_band_time_stack(stack_masked_band_time_series, quantiles):\n",
    "    return stack_masked_band_time_series.quantile(\n",
    "        q=quantiles, dim=\"time\"\n",
    "    )  # needed if we do not do .compute().chunk(None) before calling this ufunc .chunk(dict(time=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b8fa0-daaa-4b58-8448-df565fb1b7d1",
   "metadata": {},
   "source": [
    "## Input data and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8c0169-a1c1-4e50-aea1-f0c7ba464364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input data\n",
    "# copy of a geopandas.GeoDataFrame().to_json()\n",
    "# geojson_chips = '{\"type\": \"FeatureCollection\", \"features\": [{\"id\": \"2\", \"type\": \"Feature\", \"properties\": {\"chip_id\": 2, \"easting\": 688503.1824475648, \"epsg\": 32632, \"id\": \"Lerchenauer See\", \"latitude\": 48.19723, \"longitude\": 11.53678, \"maxx\": 689460.0, \"maxy\": 5342280.0, \"minx\": 687600.0, \"miny\": 5340420.0, \"northing\": 5341333.603224352, \"tiles\": \"32UPU\", \"zone_letter\": \"U\", \"zone_number\": 32}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[11.524234094137505, 48.18928587085897], [11.525055996690533, 48.20600379648934], [11.550065587909714, 48.205451427901956], [11.549235566161816, 48.18873382438354], [11.524234094137505, 48.18928587085897]]]}}, {\"id\": \"3\", \"type\": \"Feature\", \"properties\": {\"chip_id\": 3, \"easting\": 705439.5861520077, \"epsg\": 32632, \"id\": \"Kieswerk Ebenhoeh\", \"latitude\": 48.18998, \"longitude\": 11.76433, \"maxx\": 706380.0, \"maxy\": 5342040.0, \"minx\": 704520.0, \"miny\": 5340180.0, \"northing\": 5341111.3411528235, \"tiles\": \"32UPU, 32UQU\", \"zone_letter\": \"U\", \"zone_number\": 32}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[11.751522831107586, 48.18190724189189], [11.752418453742267, 48.198622128828674], [11.777419839448665, 48.19802037502167], [11.77651610687036, 48.18130583892132], [11.751522831107586, 48.18190724189189]]]}}, {\"id\": \"4\", \"type\": \"Feature\", \"properties\": {\"chip_id\": 4, \"easting\": 706949.9406789518, \"epsg\": 32632, \"id\": \"Marzlinger Weiher\", \"latitude\": 48.392441, \"longitude\": 11.795693, \"maxx\": 707880.0, \"maxy\": 5364600.0, \"minx\": 706020.0, \"miny\": 5362740.0, \"northing\": 5363696.487794535, \"tiles\": \"32UPU, 32UQU\", \"zone_letter\": \"U\", \"zone_number\": 32}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[11.782676865182088, 48.38415047868298], [11.783589070509988, 48.40086433526531], [11.808688737925511, 48.400253896920304], [11.807768333467632, 48.38354039650945], [11.782676865182088, 48.38415047868298]]]}}, {\"id\": \"5\", \"type\": \"Feature\", \"properties\": {\"chip_id\": 5, \"easting\": 704137.0853263629, \"epsg\": 32632, \"id\": \"Tegernsee\", \"latitude\": 47.7421, \"longitude\": 11.72314, \"maxx\": 705060.0, \"maxy\": 5292180.0, \"minx\": 703200.0, \"miny\": 5290320.0, \"northing\": 5291228.066208213, \"tiles\": \"32UPU, 32UQU, 32TPT, 32TQT\", \"zone_letter\": \"T\", \"zone_number\": 32}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[11.710230094604286, 47.734234541181756], [11.711098555200053, 47.750951312207526], [11.735885627074119, 47.75036268068942], [11.735009250336178, 47.73364625232421], [11.710230094604286, 47.734234541181756]]]}}, {\"id\": \"1\", \"type\": \"Feature\", \"properties\": {\"chip_id\": 1, \"easting\": 321827.8428315255, \"epsg\": 32633, \"id\": \"Raedlinger Weiher\", \"latitude\": 48.67412, \"longitude\": 12.5797, \"maxx\": 322740.0, \"maxy\": 5394960.0, \"minx\": 320880.0, \"miny\": 5393100.0, \"northing\": 5394057.097320923, \"tiles\": \"32UQU, 32UQV, 33UUP, 33UUQ\", \"zone_letter\": \"U\", \"zone_number\": 33}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[12.567250916755459, 48.66524646552912], [12.5664454366049, 48.68196415944526], [12.591692294447302, 48.68249501125058], [12.59248943795, 48.66577700713926], [12.567250916755459, 48.66524646552912]]]}}]}'\n",
    "# chips = gpd.read_file(io.StringIO(geojson_chips))\n",
    "chips = gpd.read_file(\"sol_chem_pnts_horizons_africa_chip_geometries.gpkg\")\n",
    "\n",
    "# # parameters\n",
    "valid_scl_ids = [2, 4, 5, 6]\n",
    "# must contain bands and the SCL layer\n",
    "bands = [\"B02\", \"B04\", \"B8A\", \"B09\", \"B10\", \"B11\", \"B12\", \"SCL\"]\n",
    "datetimes_all_seasons = \"2018-01-01/2019-12-31\"\n",
    "# seasons with breaks, e.g.\n",
    "seasons = {\n",
    "    \"season_1\": [\n",
    "        \"2018-01-01/2018-03-31\",\n",
    "        \"2018-07-01/2018-09-30\",\n",
    "        \"2019-01-01/2019-03-31\",\n",
    "        \"2019-07-01/2019-09-30\",\n",
    "    ],\n",
    "    \"season_2\": [\n",
    "        \"2018-04-01/2018-06-30\",\n",
    "        \"2018-10-01/2018-12-31\",\n",
    "        \"2019-04-01/2019-06-30\",\n",
    "        \"2019-10-01/2019-12-31\",\n",
    "    ],\n",
    "}\n",
    "quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "add_mid50 = True\n",
    "add_mid80 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74608fd6-71b5-46db-a6d4-965fdb065ed2",
   "metadata": {},
   "source": [
    "## Cluster\n",
    "\n",
    "https://planetarycomputer.microsoft.com/docs/quickstarts/scale-with-dask/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fc7010-b4a1-4038-89cd-7ba99ebbca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pccompute.westeurope.cloudapp.azure.com/compute/services/dask-gateway/clusters/prod.6aec14f833d541d08fd8b62b8a8b115b/status\n"
     ]
    }
   ],
   "source": [
    "import dask_gateway\n",
    "\n",
    "cluster = dask_gateway.GatewayCluster()\n",
    "client = cluster.get_client()\n",
    "cluster.scale(5)\n",
    "print(cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a719ab-beaa-41fc-a86e-441483dcfc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8135150c-332f-430b-aaa0-51cebb1fb78e",
   "metadata": {},
   "source": [
    "## Target Storage\n",
    "\n",
    "### Azure Blob\n",
    "\n",
    "Required for methods\n",
    "* `Chip.upload_quartiles_as_netcdf_to_blob`\n",
    "* `Chip.upload_scenes_csv_to_blob`\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import azure.storage.blob\n",
    "import io\n",
    "\n",
    "connection_string = getpass.getpass()  # prompts for the connection string\n",
    "container_client = azure.storage.blob.ContainerClient.from_connection_string(\n",
    "    connection_string, container_name=\"test\"\n",
    ")\n",
    "\n",
    "# to list and delete all files ...\n",
    "blob_names = [b.name for b in container_client.list_blobs()]\n",
    "# container_client.delete_blobs(*blob_names)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7d100-bca2-49ae-85ae-3977303132e7",
   "metadata": {},
   "source": [
    "### AWS S3\n",
    "\n",
    "\n",
    "Required for methods\n",
    "* `Chip.upload_quartiles_as_netcdf_to_bucket`\n",
    "* `Chip.upload_scenes_csv_to_bucket`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893fbcaf-be0d-4331-a03d-fdf0c0a031fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n",
      " ········\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import getpass\n",
    "\n",
    "access_key = getpass.getpass()\n",
    "secret_key = getpass.getpass()\n",
    "\n",
    "\n",
    "# Create connection to S3\n",
    "s3 = boto3.resource(\n",
    "    \"s3\", aws_access_key_id=access_key, aws_secret_access_key=secret_key\n",
    ")\n",
    "\n",
    "# # Get bucket object\n",
    "bucket_name = \"mi4people-soil-project\"\n",
    "basedir_chips = \"chips/s2_metrics_p/\"\n",
    "boto_bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcf85db5-7499-4fce-b18b-60345f7e522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get what is already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d0b0591-cf8f-4a26-921d-c4cbcdf66d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_files_using_paginator(bucket_name, prefix, access_key, secret_key):\n",
    "    \"\"\"\n",
    "    This functions list all files in s3 using paginator.\n",
    "    Paginator is useful when you have 1000s of files in S3.\n",
    "    S3 list_objects_v2 can list at max 1000 files in one go.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\", aws_access_key_id=access_key, aws_secret_access_key=secret_key\n",
    "    )\n",
    "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "    response_iterator = paginator.paginate(\n",
    "        Bucket=bucket_name,\n",
    "        Prefix=prefix,\n",
    "    )\n",
    "    all_files = []\n",
    "    for page in response_iterator:\n",
    "        files = page.get(\"Contents\")\n",
    "        for file in files:\n",
    "            all_files.append(file[\"Key\"])\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5501894f-39e2-4b07-bf18-9e0f24d1aaf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process all chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4004894b-645e-4033-93ad-92f7e00bdcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "class ChipPercentiles:\n",
    "    def __init__(\n",
    "        self,\n",
    "        chip,\n",
    "        bands=[\"B02\", \"B04\", \"B8A\", \"B09\", \"B10\", \"B11\", \"B12\", \"SCL\"],\n",
    "        seasons={\n",
    "            \"season_1\": [\n",
    "                \"2018-01-01/2018-03-31\",\n",
    "                \"2018-07-01/2018-09-30\",\n",
    "                \"2019-01-01/2019-03-31\",\n",
    "                \"2019-07-01/2019-09-30\",\n",
    "            ],\n",
    "            \"season_2\": [\n",
    "                \"2018-04-01/2018-06-30\",\n",
    "                \"2018-10-01/2018-12-31\",\n",
    "                \"2019-04-01/2019-06-30\",\n",
    "                \"2019-10-01/2019-12-31\",\n",
    "            ],\n",
    "        },\n",
    "        quantiles=[0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "        add_mid50=True,\n",
    "        add_mid80=True,\n",
    "        datetimes_all_seasons=\"2018-01-01/2019-12-31\",\n",
    "        valid_scl_ids=[2, 4, 5, 6],\n",
    "    ):\n",
    "\n",
    "        # used in query\n",
    "        self.chip = chip\n",
    "        self.bands = bands\n",
    "        self.datetimes_all_seasons = datetimes_all_seasons\n",
    "\n",
    "        # used for cloud masking bands based on SCL\n",
    "        self.valid_scl_ids = valid_scl_ids\n",
    "\n",
    "        # used to calculate the percentiles\n",
    "        self.seasons = seasons\n",
    "        self.quantiles = quantiles\n",
    "        self.add_mid50 = add_mid50\n",
    "        self.add_mid80 = add_mid80\n",
    "\n",
    "        #\n",
    "\n",
    "        # from get_expected_files\n",
    "        self.expected_files = None\n",
    "\n",
    "        # from get_items\n",
    "        self.items = None\n",
    "        self.candidate_scenes = None\n",
    "\n",
    "        # from crate_stack_bands_masked\n",
    "        self.stack_bands_masked = None\n",
    "\n",
    "        # from create_quartiles\n",
    "        self.ds_quartiles_all_seasons = None\n",
    "\n",
    "    def get_filename_single_layer_tiff(self, season_name, band, quantile):\n",
    "        try:\n",
    "            q_name = f\"{int(quantile*100):03d}\"\n",
    "        except:\n",
    "            q_name = quantile\n",
    "        filename = f\"chip-{self.chip.chip_id:05d}_s2_season-{season_name.lower()}_band-{band.lower()}_q-{q_name}.tif\"\n",
    "        return filename\n",
    "\n",
    "    def get_items(self):\n",
    "        # get all deduplicated items matching the query parameters\n",
    "        self.items, self.candidate_scenes = get_items(\n",
    "            pystac_lient_url=\"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "            datetimes=self.datetimes_all_seasons,\n",
    "            chip=self.chip,\n",
    "        )\n",
    "\n",
    "    def crate_stack_bands_masked(self):\n",
    "\n",
    "        # from the items create a xarray DataArray with dims ('time', 'band', 'y', 'x')\n",
    "        stack = stackstac.stack(\n",
    "            planetary_computer.sign(pystac.ItemCollection(self.items)),\n",
    "            assets=self.bands,\n",
    "            bounds=[self.chip.minx, self.chip.miny, self.chip.maxx, self.chip.maxy],\n",
    "        )\n",
    "        # stack containing only the scene classification layer of all scenes\n",
    "        stack_scl = stack.drop_sel(band=\"SCL\")\n",
    "        # stack with a mask where all valid pixes are Treu and invalid pixes are False\n",
    "        stack_valid_pixel_mask = stack.sel(band=\"SCL\").isin(self.valid_scl_ids)\n",
    "        # stack with only the B* bands\n",
    "        stack_bands = stack.drop_sel(band=\"SCL\")\n",
    "        # stack with only the B* bands masked, i.d. with nan where the pixels are invalid\n",
    "\n",
    "        # we drop ['gsd', 'common_name'] to avoid the ValueError: Variables {'gsd', 'common_name'} are coordinates in some datasets but not others.\n",
    "        self.stack_bands_masked = (\n",
    "            (\n",
    "                stack_bands.drop_vars([\"gsd\", \"common_name\"])\n",
    "                .groupby(\"band\")\n",
    "                .map(\n",
    "                    mask_bands_with_valid_pixel_mask,\n",
    "                    stack_valid_pixel_mask=stack_valid_pixel_mask,\n",
    "                )\n",
    "            )\n",
    "            .compute()\n",
    "            .chunk({})\n",
    "        )\n",
    "\n",
    "    def create_quartiles(self):\n",
    "\n",
    "        ds_quartiles_all_seasons = []\n",
    "        for season_name, season_periods in self.seasons.items():\n",
    "\n",
    "            # time subset the stack for the given season\n",
    "            # get the times in the stack that fall in any of the periods of the given season\n",
    "            season_times = filter_times_given_periods(\n",
    "                self.stack_bands_masked.time.values, season_periods\n",
    "            )\n",
    "            # print(f'Time filter for season {season_name} returned {len(season_times)} of {len(stack_bands_masked.time.values)}.')\n",
    "            # subset the stack\n",
    "            stack_bands_masked_season = self.stack_bands_masked[\n",
    "                self.stack_bands_masked.time.isin(season_times)\n",
    "            ]\n",
    "\n",
    "            # calculate quantiles\n",
    "            stack_bands_masked_season_quantiles = stack_bands_masked_season.groupby(\n",
    "                \"band\"\n",
    "            ).map(quantiles_from_masked_band_time_stack, quantiles=self.quantiles)\n",
    "\n",
    "            # add dispersion measures\n",
    "            # The IQR may also be called the midspread, middle 50%, fourth spread, or H‑spread.\n",
    "            # https://en.wikipedia.org/wiki/Interquartile_range\n",
    "            if self.add_mid50:\n",
    "                mid50 = stack_bands_masked_season_quantiles.sel(\n",
    "                    quantile=0.75\n",
    "                ) - stack_bands_masked_season_quantiles.sel(quantile=0.25)\n",
    "                stack_bands_masked_season_quantiles = xr.concat(\n",
    "                    [\n",
    "                        stack_bands_masked_season_quantiles,\n",
    "                        mid50.expand_dims(quantile=[\"mid50\"]),\n",
    "                    ],\n",
    "                    dim=\"quantile\",\n",
    "                )\n",
    "            if self.add_mid80:\n",
    "                mid80 = stack_bands_masked_season_quantiles.sel(\n",
    "                    quantile=0.9\n",
    "                ) - stack_bands_masked_season_quantiles.sel(quantile=0.1)\n",
    "                stack_bands_masked_season_quantiles = xr.concat(\n",
    "                    [\n",
    "                        stack_bands_masked_season_quantiles,\n",
    "                        mid80.expand_dims(quantile=[\"mid80\"]),\n",
    "                    ],\n",
    "                    dim=\"quantile\",\n",
    "                )\n",
    "\n",
    "            # add crs and convert to desired output data type\n",
    "            stack_bands_masked_season_quantiles = (\n",
    "                stack_bands_masked_season_quantiles.rio.write_crs(\n",
    "                    rioxarray.crs.CRS.from_epsg(self.chip[\"epsg\"]).to_string(),\n",
    "                    inplace=False,\n",
    "                )\n",
    "                .astype(\"uint16\")\n",
    "                .to_dataset(\"band\")\n",
    "                .expand_dims({\"time_period\": [season_name]})\n",
    "            )\n",
    "            ds_quartiles_all_seasons.append(stack_bands_masked_season_quantiles)\n",
    "        ds_quartiles_all_seasons = xr.concat(\n",
    "            ds_quartiles_all_seasons, dim=\"time_period\"\n",
    "        )\n",
    "        # we need to make sure that the whole\n",
    "        new_coords = [\n",
    "            str(f\"p{int(q*100):03d}\") if isinstance(q, float) else f\"p{q}\"\n",
    "            for q in ds_quartiles_all_seasons.coords.get(\"quantile\").values\n",
    "        ]\n",
    "        ds_quartiles_all_seasons = ds_quartiles_all_seasons.assign_coords(\n",
    "            quantile=new_coords\n",
    "        ).rename({\"quantile\": \"metric\"})\n",
    "        self.ds_quartiles_all_seasons = ds_quartiles_all_seasons\n",
    "\n",
    "    def load_quartiles_in_memory(self):\n",
    "        # we need to load result in memory else we get the following during to_netcdf\n",
    "        # ValueError: invalid engine for creating bytes with to_netcdf: 'netcdf4'. Only the default engine or engine='scipy' is supported\n",
    "        self.ds_quartiles_all_seasons_in_memory = (\n",
    "            self.ds_quartiles_all_seasons.compute()\n",
    "        )\n",
    "\n",
    "    def upload_quartiles_as_netcdf_to_blob(self, container_client):\n",
    "        filename = self.get_expected_filename_single_netcdf_file()\n",
    "        with io.BytesIO() as buffer:\n",
    "            buffer.write(self.ds_quartiles_all_seasons_in_memory.to_netcdf())\n",
    "            buffer.seek(0)\n",
    "            blob_client = container_client.get_blob_client(filename)\n",
    "            blob_client.upload_blob(buffer, overwrite=True)\n",
    "\n",
    "    def upload_scenes_csv_to_blob(self, container_client, filename):\n",
    "        # filename = self.get_expected_filename_scenes_csv_file()\n",
    "        with io.BytesIO() as buffer:\n",
    "            self.candidate_scenes.to_csv(buffer)\n",
    "            buffer.seek(0)\n",
    "            blob_client = container_client.get_blob_client(filename)\n",
    "            blob_client.upload_blob(buffer, overwrite=True)\n",
    "\n",
    "    def upload_quartiles_as_netcdf_to_bucket(self, boto_bucket, filename):\n",
    "        # filename = self.get_expected_filename_single_netcdf_file()\n",
    "        with io.BytesIO() as buffer:\n",
    "            buffer.write(self.ds_quartiles_all_seasons_in_memory.to_netcdf())\n",
    "            buffer.seek(0)\n",
    "            boto_bucket.upload_fileobj(buffer, filename)\n",
    "\n",
    "    def upload_scenes_csv_to_bucket(self, boto_bucket, filename):\n",
    "        # filename = self.get_expected_filename_scenes_csv_file()\n",
    "        with io.BytesIO() as buffer:\n",
    "            self.candidate_scenes.to_csv(buffer)\n",
    "            buffer.seek(0)\n",
    "            boto_bucket.upload_fileobj(buffer, filename)\n",
    "\n",
    "    def write_quartiles_as_netcdf_locally(self, filename):\n",
    "        # filename = dst_dir + self.get_expected_filename_single_netcdf_file()\n",
    "        self.ds_quartiles_all_seasons_in_memory.to_netcdf(filename)\n",
    "\n",
    "    def write_scenes_csv_locally(self, filename):\n",
    "        # filename = dst_dir + self.get_expected_filename_scenes_csv_file()\n",
    "        self.candidate_scenes.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24b9c4-facc-4166-8ad9-d5cd556607a4",
   "metadata": {},
   "source": [
    "## Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4447d4b3-44d6-4d82-a191-a75bc81fc23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chip_s = chips_to_be_processed.iloc[2]\n",
    "# # chip_s = chips.iloc[0]\n",
    "\n",
    "# chip = ChipPercentiles(\n",
    "#     chip_s,\n",
    "#     bands=bands,\n",
    "#     seasons=seasons,\n",
    "#     quantiles=quantiles,\n",
    "#     add_mid50=True,\n",
    "#     add_mid80=True,\n",
    "#     datetimes_all_seasons=datetimes_all_seasons,\n",
    "#     valid_scl_ids=valid_scl_ids,\n",
    "# )\n",
    "\n",
    "# chip.get_items()\n",
    "# chip.crate_stack_bands_masked()\n",
    "# chip.create_quartiles()\n",
    "# chip.load_quartiles_in_memory()\n",
    "\n",
    "# # save\n",
    "# olc_id = chip.chip.olc_id\n",
    "\n",
    "# # str(Path(...) ...) is safer\n",
    "# # if we mess up the separaters (zero or two / instead of one)\n",
    "# # aws will take it serious and create kindo of a nameless directory\n",
    "# filename_nc = str(Path(basedir_chips) / 'data' / f\"{olc_id}.nc\")\n",
    "# filename_s2_scenes = str((Path(basedir_chips) / 'meatadata_s2_scenes' / f\"{olc_id}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21cc08df-f1b6-4476-8259-f064803d0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chip.upload_quartiles_as_netcdf_to_bucket(boto_bucket=boto_bucket, filename=filename_nc)\n",
    "# chip.upload_scenes_csv_to_bucket(boto_bucket=boto_bucket, filename=filename_s2_scenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ddcba",
   "metadata": {},
   "source": [
    "## Run for all chips\n",
    "\n",
    "### Get locations that needs to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0cb6ee7-92d3-408d-a045-e76e89e4657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29927, 15)\n",
      "(1069, 15)\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"mi4people-soil-project\"\n",
    "basedir_chips = \"chips/s2_metrics_p\"\n",
    "\n",
    "existing_files_in_dir = list_s3_files_using_paginator(\n",
    "    bucket_name, basedir_chips, access_key, secret_key\n",
    ")\n",
    "\n",
    "df_computed = pd.DataFrame(existing_files_in_dir, columns=[\"file\"])\n",
    "# remove directories\n",
    "df_computed = df_computed.query('~file.str.endswith(\"/\")')\n",
    "df_computed[\"olc_id\"] = df_computed[\"file\"].str.extract(r\".*/(.*)\\.\\w{2,3}\")\n",
    "df_computed\n",
    "# get all complete (csv and nc) chips\n",
    "df_computed[[\"olc_id\", \"suffix\"]] = df_computed[\"file\"].str.extract(\n",
    "    r\".*/(.*)\\.(\\w{2,3})\"\n",
    ")\n",
    "assert df_computed[\"suffix\"].isin([\"csv\", \"nc\"]).all()\n",
    "olc_ids_computed_bool = df_computed[\"olc_id\"].value_counts() == 2\n",
    "olc_ids_computed = olc_ids_computed_bool[olc_ids_computed_bool].index.values\n",
    "\n",
    "chips_to_be_processed = chips[~chips[\"olc_id\"].isin(olc_ids_computed)]\n",
    "print(chips.shape)\n",
    "print(chips_to_be_processed.shape)\n",
    "\n",
    "# 2022-10-24\n",
    "# (29927, 15)\n",
    "# (27994, 15)\n",
    "\n",
    "# (29927, 15)\n",
    "# (24141, 15)\n",
    "\n",
    "# (29927, 15)\n",
    "# (23579, 15)\n",
    "\n",
    "# (29927, 15)\n",
    "# (19805, 15)\n",
    "\n",
    "# (29927, 15)\n",
    "# (16249, 15)\n",
    "\n",
    "# (29927, 15)\n",
    "# (12757, 15)\n",
    "\n",
    "# (29927, 15)\n",
    "# (9478, 15)\n",
    "\n",
    "# (29927, 15)\n",
    "# (6078, 15)\n",
    "\n",
    "# (29927, 15)\n",
    "# (2525, 15)\n",
    "\n",
    "# (29927, 15)\n",
    "# (1069, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18ba56-d33a-49ac-b2f2-db57ee5ef24e",
   "metadata": {},
   "source": [
    "### Remove locations where we did not get any scenes for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d0afa05-0c4c-4eef-9bbe-ce3980b9d8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7CGCM2C4+9R7', '7C4PC228+2M3', '7C4P95MX+8XQ', '7C4P86MM+88P', '7C4PF528+2M3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1069"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chips_no_data = []\n",
    "with open(\"s2-chips-generation.log\") as logfile:\n",
    "    for line in logfile.readlines():\n",
    "        if \"No scenes found\" in line:\n",
    "            chips_no_data.append(line.split(\" \")[-1].replace(\"\\n\", \"\"))\n",
    "\n",
    "print(chips_no_data[:5])\n",
    "len(chips_no_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75f8afa9-2161-472a-9f64-ee4bf6fa8246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1069, 15)\n",
      "(0, 15)\n"
     ]
    }
   ],
   "source": [
    "print(chips_to_be_processed.shape)\n",
    "print(chips_to_be_processed.query(f\"~olc_id.isin({chips_no_data})\").shape)\n",
    "chips_to_be_processed = chips_to_be_processed.query(f\"~olc_id.isin({chips_no_data})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753fa7a-e65a-4d54-b34e-25ba29f8773c",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92aa709c-59c3-491d-82a7-b03745ce08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Create a custom logger\n",
    "\n",
    "logger = logging.getLogger()\n",
    "while logger.hasHandlers():\n",
    "    logger.removeHandler(logger.handlers[0])\n",
    "\n",
    "# Create handlers\n",
    "f_handler = logging.FileHandler(\"s2-chips-generation.log\")\n",
    "f_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatters and add it to handlers\n",
    "f_format = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "f_handler.setFormatter(f_format)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(f_handler)\n",
    "\n",
    "# try:\n",
    "#     raise RuntimeError(\"Error opening 'https://sentinel2l2a01.blob.core.windows.net/sentinel2-l2/29/P/KQ/2018/01/02/S2B_MSIL2A_20180102T111439_N0212_R137_T29PKQ_20201014T025243.SAFE/GRANULE/L2A_T29PKQ_A004310_20180102T111901/IMG_DATA/R10m/T29PKQ_20180102T111439_B02_10m.tif?st=2022-10-02T06%3A19%3A11Z&se=2022-10-10T06%3A19%3A11Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-10-03T06%3A19%3A10Z&ske=2022-10-10T06%3A19%3A10Z&sks=b&skv=2021-06-08&sig=cPPukdFO//xFki6IB8wwIza6SpYCwqsc5uHnkrgm8rA%3D': RasterioIOError('HTTP response code: 403\")\n",
    "# except RuntimeError as ex:\n",
    "#     url = ex.args[0].split(\"'\")[1]\n",
    "#     se = datetime.datetime.strptime(urllib.parse.parse_qs(url)[\"se\"][0], \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=datetime.timezone.utc)\n",
    "#     logger.error(f'TEST !!! UTC time of RuntimeError / SE | {datetime.datetime.utcnow()} / {se}', exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c893e46-40ea-4999-8c56-8ca525764905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import datetime\n",
    "\n",
    "# overwrite = True\n",
    "# blob_names = [b.name for b in container_client.list_blobs()]\n",
    "\n",
    "scenes_not_found = []\n",
    "for chip_index, chip_s in tqdm(\n",
    "    chips_to_be_processed.iterrows(), total=chips_to_be_processed.shape[0]\n",
    "):\n",
    "    try:\n",
    "        chip = ChipPercentiles(\n",
    "            chip_s,\n",
    "            bands=bands,\n",
    "            seasons=seasons,\n",
    "            quantiles=quantiles,\n",
    "            add_mid50=True,\n",
    "            add_mid80=True,\n",
    "            datetimes_all_seasons=datetimes_all_seasons,\n",
    "            valid_scl_ids=valid_scl_ids,\n",
    "        )\n",
    "        olc_id = chip.chip.olc_id\n",
    "\n",
    "        chip.get_items()\n",
    "        if chip.candidate_scenes is None:\n",
    "            # print(f'No scenes found for {chip.chip.olc_id}')\n",
    "            scenes_not_found.append(olc_id)\n",
    "            logger.warning(f\"No scenes found for {chip.chip.olc_id}\")\n",
    "            continue\n",
    "        chip.crate_stack_bands_masked()\n",
    "        chip.create_quartiles()\n",
    "        chip.load_quartiles_in_memory()\n",
    "\n",
    "        # str(Path(...) ...) is safer\n",
    "        # if we mess up the separaters (zero or two / instead of one)\n",
    "        # aws will take it serious and create kindo of a nameless directory\n",
    "        filename_nc = str(Path(basedir_chips) / \"data\" / f\"{olc_id}.nc\")\n",
    "        filename_s2_scenes = str(\n",
    "            (Path(basedir_chips) / \"meatadata_s2_scenes\" / f\"{olc_id}.csv\")\n",
    "        )\n",
    "\n",
    "        chip.upload_quartiles_as_netcdf_to_bucket(\n",
    "            boto_bucket=boto_bucket, filename=filename_nc\n",
    "        )\n",
    "        chip.upload_scenes_csv_to_bucket(\n",
    "            boto_bucket=boto_bucket, filename=filename_s2_scenes\n",
    "        )\n",
    "        logger.info(f\"Chip processed {chip.chip.olc_id}\")\n",
    "\n",
    "    except RuntimeError:\n",
    "        url = ex.args[0].split(\"'\")[1]\n",
    "        se = datetime.datetime.strptime(\n",
    "            urllib.parse.parse_qs(url)[\"se\"][0], \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "        ).replace(tzinfo=datetime.timezone.utc)\n",
    "        logger.error(\n",
    "            f\"UTC time of RuntimeError / SE | {datetime.datetime.utcnow()} / {se}\",\n",
    "            exc_info=True,\n",
    "        )\n",
    "\n",
    "    except Exception as ex:\n",
    "        logger.error(f\"Exception ocurred\", exc_info=True)\n",
    "\n",
    "#   0%|          | 5/28051 [02:21<220:30:28, 28.30s/it]\n",
    "# 2022-10-23 14:43:39,675 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81e5d2f46d9fa1703a21b89b5a639cc71eabd8a8e9cafc9cb66456107252e12e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
